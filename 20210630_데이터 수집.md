#### 1. 크롤러(crawler)

- 자동으로 웹 페이지에 있는 정보를 수집하는 프로그램이다.

#### 2. 스크레이핑(scraping)

- 수집한 정보를 분석해서 필요한 정보를 추출하는 기술

#### 3. 파이썬에서 http request를 다루는 모듈-url에서 html소스가져오기

- urllib 모듈 : 파이썬의 내장 모듈
- requests 모듈 : 외부 모듈

 ####  4. Python에서 스파이더링(크롤링)

- BeautifulSoup



### urllib 모듈

```python
import urllib.request
from urllib.request import urlopne
import requests
```

```python
url = 'https://www.naver.com'
html = urlopen(url)
print(html.status) # 응답코드 200 (정상적으로 응답받음)
urllib.request.urlretrieve(url, './urllib_naver.html') # html파일로 저장
```



### requests 모듈

```python
url = 'https://www.naver.com'
cont = requests.get(url) # Response [200]
print(cont)
print(cont.staus_code) # 200
```



### BeauifulSoup

```python
import requests
import bs4

url = 'https://www.naver.com'

res = requests.get(url)

# BeautifulSoup(parsing할 데이터, parsing 방법 )
bs_obj = bs4.BeauifulSoup(res.text)
print(bs_obj.find('div')) # div에 대한 최초 정보를 가져온다

# res.text를 html으로 파싱
bs_obj2 = bs4.BeautifulSoup(res.text, 'html.parser')
print(bs_obj2.find('div'))

# res.text를 html으로 파싱 (이것을 더많이 사용한다 더빠름)
bs_obj3 = bs4.BeautifulSoup(res.text, 'lxml')
print(bs_obj3.find('div'))
```



### .find()

```python
import bs4

html_str = '<html><div>Hello</div></html>'
bs_obj = bs4.BeautifulSoup(html_str, 'lxml')
print(bs_obj) # '<html><div>Hello</div></html>'
print(bs_obj.find('div')) # <div>Hello</div>
print(bs_obj.find('div').text) # Hello
```



### find_all() vs findAll()

- findAll() : BeautifulSoup 3버전까지 사용했음

```python
import bs4

html_str = """
<html>
    <body>
        <ul>
            <li>hello</li>
            <li>bye</li>
            <li>welcome</li>
        </ul>
    </body>
</html>
"""

bs_obj = bs4.BeautifulSoup(html_str, 'lxml')

ul = bs_obj.find('ul')
ul_li = ul.find_all('li')
print(ul_li) # [<li>hello</li>, <li>bye</li>, <li>welcome</li>]

print(ul_li[0]) # <li>hello</li>
print(ul_li[1]) # <li>bye</li>
print(ul_li[2]) # <li>welcome</li>

for element in ul_li:
    print(element.text)
```



### 데이터 뽑을때 class 속성 이용하기 tag_and_property

```python
import bs4

html_str = """
<html>
    <body>
        <ul class="greet">
            <li>hello</li>
            <li>bye</li>
            <li>welcome</li>
        </ul> 
        <ul class="reply">
            <li>ok</li>
            <li>no</li>
            <li>sure</li>
        </ul>
    </body>
</html>
"""

bs_obj = bs4.BeautifulSoup(html_str, 'lxml')

ul_reply = bs_obj.find('ul')
print(ul_reply)

> <ul class="greet">
<li>hello</li>
<li>bye</li>
<li>welcome</li>
</ul>

ul_reply = bs_obj.find('ul', attrs={'class': 'reply'})
print(ul_reply)

> <ul class="reply">
<li>ok</li>
<li>no</li>
<li>sure</li>
</ul>

ul_li = ul_reply.find_all('li')

for li in ul_li : 
    print(li.text)
    
> ok
no
sure
```



### 속성값 뽑아내기 property_herf

```python
import bs4

html_str = """
<html>
    <body>
        <ul class="ko">
            <li>
                <a href="https://www.naver.com/">네이버</a>
            </li>
            <li>
                <a href="https://www.daum.net/">다음</a>
            </li>
        </ul>
        <ul class="sns">
            <li>
                <a href="https://www.google.com/">구글</a>
            </li>
            <li>
                <a href="https://www.facebook.com/">페이스북</a>
            </li>
        </ul>
    </body>
</html>
"""

bs_obj = bs4.BeautifulSoup(html_str, 'lxml')
atag = bs_obj.find('a')
print(atag)
print(atag['href'])

> <a href="https://www.naver.com/">네이버</a>
https://www.naver.com/
    
bs_obj = bs4.BeautifulSoup(html_str, 'lxml')
atag = bs_obj.find_all('a')
print(atag)

for attr in atag:
    print(attr['href'])
    
> [<a href="https://www.naver.com/">네이버</a>, <a href="https://www.daum.net/">다음</a>, <a href="https://www.google.com/">구글</a>, <a href="https://www.facebook.com/">페이스북</a>]
https://www.naver.com/
https://www.daum.net/
https://www.google.com/
https://www.facebook.com/
```

```python
# single element
print(bs_obj.find('a'))
print(bs_obj.a)

print(bs_obj.find('a').get_text())
print(bs_obj.a.get_text())

> <a href="https://www.naver.com/">네이버</a>
<a href="https://www.naver.com/">네이버</a>
네이버
네이버
```

```python
# multi element
print(bs_obj.find_all('a')
      
print(bs_obj.find_all('a')[0].get_text())
print(bs_obj.find_all('a')[1].get_text())
      
> [<a href="https://www.naver.com/">네이버</a>, <a href="https://www.daum.net/">다음</a>, <a href="https://www.google.com/">구글</a>, <a href="https://www.facebook.com/">페이스북</a>]
네이버
다음

# 둘 타입이 달라서 주의해야한다
print(type(bs_obj.find_all('a'))) # <class 'bs4.element.ResultSet'>
print(type(bs_obj.a)) # <class 'bs4.element.Tag'>
```



### select_one() 과 select()

```python
import bs4

html_str = """
<html>
    <body>
        <ul class="ko">
            <li>
                <a href="https://www.naver.com/">네이버</a>
            </li>
            <li>
                <a href="https://www.daum.net/">다음</a>
            </li>
        </ul>
        <ul class="sns">
            <li>
                <a href="https://www.google.com/">구글</a>
            </li>
            <li>
                <a href="https://www.facebook.com/">페이스북</a>
            </li>
        </ul>
        <div id = 'result'>page</div>
    </body>
</html>
"""

bs_obj = bs4.BeautifulSoup(html_str, 'lxml')

ul_li = bs_obj.select_one('ul')
print(ul)

print('------------------------')
ul_li = bs_obj.select_one('ul>li')
print(ul_li)

> <ul>
<li>hello</li>
<li>bye</li>
<li>welcome</li>
</ul>
------------------------
<li>
<a href="https://www.naver.com/">네이버</a>
</li>
```



### 응답코드가 406일때 문제해결 방법

- 문제해결 방법
  - requests.get(url,headers=header)

```python
import bs4
import requests

url = 'https://www.melon.com/'
header ={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'} 
res = requests.get(url, headers = header)
print(res) # <Response [406]>

bs_melon = bs4.BeatuifulSoup(res.text, 'lxml')
print(bs_melon)
```



## 크롤링 허용 여부 확인하기

- 크롤링주소/robots.txt(https://www.daum.net/robots.txt)

- 만약에 https://www.naver.com/robot.txt 실행했을때 robots.txt파일이 검색이 안되면 수집에 대한 정책이 없으면 크롤링 할 수 있다고 받아들여도 된다

- 수집정책 사이트 : http://www.robotstxt.org/

  |             표시 |                  허용여부 |
  | ---------------: | ------------------------: |
  |    User-agent: * |            모든 접근 허용 |
  |        Disallow: |                           |
  |             또는 |                           |
  |    User-agent: * |                           |
  |         Allow: / |                           |
  |     User-agent:* |            모든 접근 금지 |
  |       Disallow:/ |                           |
  |    User-agent: * | 특정 디렉토리만 접근 금지 |
  | Disallow: /user/ |                           |



### GET방식 request

```python
def getDownload( url, param = None, retries = 3 ):
    resp = None
    try:
        resp = requests.get( url, params = param )
        resp.raise_for_status()
    except requests.exceptions.HTTPError as e:
        if 500 <= resp.status_code < 600 and retries > 0:
            print( 'Retries : {0}'.format( retries ) )
            return getDownload( url, param, retries - 1 )
        else:
            print( resp.status_code )
            print( resp.reason )
            print( resp.request.headers )
    return resp
```

```python
url = 'http://www.crawler-test.com/status_codes/status_500'
getDownload( url )
```



### POST 방식 request

```python
def getDownload( url, param = None, retries = 3 ):
    resp = None
    try:
        resp = requests.get( url, params = param )
        resp.raise_for_status()
    except requests.exceptions.HTTPError as e:
        if 500 <= resp.status_code < 600 and retries > 0:
            print( 'Retries : {0}'.format( retries ) )
            return getDownload( url, param, retries - 1 )
        else:
            print( resp.status_code )
            print( resp.reason )
            print( resp.request.headers )
    return resp
```

```python
url = 'http://pythonscraping.com/pages/files/processing.php'
data = { 'firstname':'테스트', 'lastname':1234 }
```



### class속성의 값이 2개 이상일때

```python
header = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}

url = requests.get('https://news.naver.com/', headers=header)
#print(url) # <Response [200]>

html = bs4.BeautifulSoup(url.text,'lxml')
#print(html)

newsnow = html.find('ul', {'class' : 'mlist2 no_bg'})
#print(newsnow)

for new_li in newsnow.find_all('li'):
    #print(li)
    new_strong = new_li.find('strong')
    #print(new_strong)
    #print(new_strong.text)
    print(re.findall('[a-z가-힣]+', new_strong.text))
```

