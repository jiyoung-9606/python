### 딥러닝



#### 딥러닝

- 인공지능 범주 안에 머신러닝이 속함
- 머신러닝의 일부분이 딥러닝



#### 딥

- 연속된 층으로 표현을 학습한다는 개념
- 데이터로부터 모델을 만드는데 얼마나 많은 층을 사용했는지가 그 모델의 깊이
- 최근의 딥러닝 모델은 표현 학습을 위해 수십 개, 수백 개의 연속된 층을 가지고 있음
- 이 층들을 훈련 데이터에 노출해서 자동으로 학습시킴
- 기본 층을 겹겹이 쌓아 올려 구성된 신경망이라는 모델을 사용하여 표현 층을 학습



#### 층 (Layer)

- 신경망의 핵심 구성 요소
- 데이터 처리 필터
- 데이터가 입력되면 더 유용한 형태로 출력
- 즉, 입력된 데이터로부터 주어진 문제에 더 의미있는 표현 (representaion) 추출



#### 딥러닝

- 기술적으로는 데이터 표현을 학습하기 위한 다단계 처리 방식을 말함



#### 퍼셉트론(Perceptron)

- 인공신경망의 한 종류
- 다수의 신호(입력)을 받아서 하나의 신호(Output)를 출력
- 뉴런이 전기신호를 내보내 정보를 전달하는 것과 유사
- 뉴런의 수상돌기나 축색돌기처럼 신호를 전달하는 역할
- 퍼셉트론 가중치(weight)가 담당
- 가중치가 각가의 입력신호에 대해 부여되어 입력신호와 계산을 하고
- 신호의 총합이 정해진 임계값을 넘으면 다음 뉴런으로 신호를 전달하고 (뉴런의 활성화)
- 아니면 아무것도 수행하지 않음
- 각 노드의 가중치와 입력치를 곱한 것을 모두 합한 값이 활성함수에 의해 판단되고
- 입력값과 활성화 함수를 사용해 출력값을 다음으로 넘기는 가장 작은 신경망 단위를 퍼셉티론이라고 함
- 가중치는 입력 신호가 결과 출력에 주는 영향도를 조절하는 매개 변수



#### 가중치, 가중합, 바이어스, 활성화 함수 기울기나 절편을 퍼셉트론의 개념에 맞게 설명

- 기울기는 퍼셉트론에서는 가중치를 의미하는 w(weight)로 표기
- 절편은 b(바이어스)로 표현 y = ax + b(a는 기울기 b는 절편) y = wx + b (w는 가중치, b는 바이어스)



#### 가중합

- 입력값(x)과 가중치(w)의 값을 모두 더한 다음 바이어스(b)를 더한 값



#### 퍼셉트론의 한계와 이를 해결하는 과정

- 퍼셉트론의 과제
- 직선하나를 그어서 검은점과 흰점을 분리하는 방법 ---> 없음
--> 이 문제는 고정관념을 깨 기발한 아이디어에서 해결점을 보였음
--> 평면을 휘어 주는 것

--> 다층 퍼셉트론



#### 다층 퍼셉트론

- 좌표 평면 자체에 변화를 주는 것
- 이를 가능하게 하려면 숨어있는 층 , 즉 은닉층(hidden layer)을 만들면 됨
- 은닉층이 좌표 평면을 왜곡시키는 결과를 가져옴
- 은닉층을 만들어 왜곡하면 두 영역을 가로지르는 선이 직선으로 바뀜
- 결과값의 오차를 구해
- 이를 토대로 앞선 가중치를 차례로 거슬러 올라가며 조정함
- 가중치 업데이터 (출력층과 은닉층의 가중치를 수정함)



#### 오차 역전파(back propagation)

- 다중 퍼셉트론을 학습시키는데 성공하면서
- XOR 문제를 해결할 수 있었고
- 다층 퍼셉트론은 단층 퍼셉트론을 결합하여 만들 것으로
- 신경망이라는 개념이 생성



#### 렐루(ReLU) 함수

- 시그모이드 함수의 대안으로 떠오르며 현재 가장 많이 사용되는 활성화 함수
- 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있음
- 이 간단한 방법으로 여러 층을 쌓을 수 있게 되었고, 이로써 딥러닝의 발전에 속도가 붙게됨
- 0 미만의 값은 모두 0으로 처리하고, 0 이상의 값은 원 값 그대로 출력
