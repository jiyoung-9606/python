### 로지스틱 회귀 (Logistic Regression)



#### 로지스틱 회귀

- 선형 회귀 방식을 분류에 적용한 알고리즘
- 선형 회귀 방식을 기반
- 시그모이드 함수를 이용해 분류 수행



#### 로지스틱 회귀가 선형 회귀와 다른점

- 학습을 통해 선형함수의 회귀선을 찾는 것이 아니라
- 시그모이드(Sigmoid)함수의 최적선을 찾고
- 이 시그모이드 함수의 반환값을 확률로 간주해
- 확률에 따라 분류를 결정



#### 시그모이드 함수

- S자 커브형태
- x값이 +,-로 아무리 커지거나 작아져도
- y값은 항상 0과 1사이의 값 반환
- x값이 커지면 1에 근사하고
- x값이 작아지면 0에 근사
- x가 0일때는 0.5
- 실제 많은 자연, 사회 현상에서
- 특정 변수의 확률값은 선형이 아니라 S자 커브 형태 가짐



#### 하이퍼 파라미터 튜닝

- GridSearchCV를 이용해 하이퍼 파라미터 최적화

#### LogistciRegression 클래스 주요 하이퍼 파라미터

- penalty : 규제 유형 설정 (l1, l2 규제) : l2가 기본
- C : 규제 강도를 조절하는 alpha값의 역수
- C = 1/alpha
- C값이 작을수록 규제 강도가 큼

- alpha : 학습 데이터 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터 

- alpha 값이 커질수록 좋음



#### 로지스틱 회귀 정리

- 가볍고 빠름
- 이진 분류 예측 성능이 뛰어남
- 이진 분류기의 기본모델로 사용하는 경우가 많음
- 희소행렬로 표현되는 텍스트 기반의 분류에서도 자주 사용됨



### 회귀 실습-캐글 주택 가격 회귀 분석 : 고급 회귀 기법 사용



#### 전체 회귀 분석 과정

(1) 데이터 전처리
(2) 선형 회귀 모델 학습/예측/평가
(3) 회귀 트리 모델 학습/예측/평가
(4) 회귀 모델의 예측 결과 혼합을 통한 최종 예측
(5) 스태킹 앙상블 모델을 통한 회귀 예측



#### (1) 데이터 전처리  

- Null
- 정규 분포인지 확인
- 로그 변환 및 환원
- 원-핫 인코딩 변환



#### 로그 변환 및 환원

- 정규 분포가 아닌 결과값을 정규 분포 형태로 변환
- log1p() 이용해 로그 변환한 결과값을 기반으로 학습한 후
- 예측 시 다시 결과값을 expm1()으로 환원해서 사용

```python
plt.title('Log Trasformed Sale Price Histogram')
log_SalePrice = np.log1p(house_df['SalePrice'])
sns.displot(log_SalePrice)
```



#### 원-핫 인코딩 변환

- 행 형태의 퍼처 값을 열 형태로 변환한 뒤
- 피처 값 인덱스에 해당하는 칼럼에는 1로 표시
- 나머지 칼럼에는 0으로 표시하는 방식
-  판다스의 get_dummies()를 이용

```python
print('get_dummies() 수행전 데이터 Shape', house_df.shape)
house_df_ohe = pd.get_dummies(house_df)
print('get_dummies() 수행 후 데이터 Shape', house_df_ohe.shape)
```



#### (2) 선형 회귀 모델 학습/예측/평가



 #### LinearRegression, Ridge, Lasso 를 이용해서 선형 계열의 회귀 모델 만들기

- LinearRegression, Ridge, Lasso 를 이용해서 선형 계열의 회귀 모델 

  ```python
  # LinearRegression, Ridge, Lasso 학습/예측/평가
  lr_reg = LinearRegression()
  lr_reg.fit(X_train, y_train)
  
  ridge_reg = Ridge()
  ridge_reg.fit(X_train, y_train)
  
  lasso_reg = Lasso()
  lasso_reg.fit(X_train, y_train)
  ```

  

- 여러 개의 회귀 모델들에 대한 회귀 계수값과 칼럼명을 시각화

- 5 폴드 교차 검증으로 모델별 RMSE와 평균 RMSE 출력

- 릿지와 라쏘 모델의 alpha 값을 변경하면서 하이퍼 파라미터 튜닝 후 다시 재학습/예측/평가 수행

- 앞의 최적화 alpha 값으로 학습 데이터로 학습, 테스트 데이터로 예측 및 평가 수행



#### 데이터 세트를 추가적으로 가공해서 모델 튜닝 좀 더 진행

- (1) 피처 데이터 세트의 데이터 분포도 확인 - 왜곡도 높은 피처들은 로그 변환 수행
- (2) 이상치 데이터 처리
  - 5 폴드 교차 검증으로 모델별 RMSE와 평균 RMSE 출력
  - 릿지와 라쏘 모델의 alpha 값을 변경하면서 하이퍼 파라미터 튜닝 후 다시 재학습/예측/평가 수행
  - 앞의 최적화 alpha 값으로 학습 데이터로 학습, 테스트 데이터로 예측 및 평가 수행



#### (3) 회귀 트리 모델 학습/예측/평가

- XGBoost와 LightGBM 이용해서 학습/예측/평가 수행
- XGBoost와 LightGBM 모두 수행 시간이 오래 걸릴 수 있는 관계로
-  하이퍼 파라미터 설정을 미리 적용한 상태로
- 5 폴트 세트에 대한 평균 RMSE 값 추출
- XGBoost 회귀 트리 적용
- LightGBM 회귀 트리 적용
- 트리 회귀 모델의 피처 중요도 시각화



#### (4) 회귀 모델의 예측 결과 혼합을 통한 최종 예측

앞에서 구한 릿지 모델(40%)과 라쏘 모델(60%) 혼합
최종 혼합 모델의 RMSE가 개별 모델보다 성능면에서 약간 개선됨



#### 스태킹 앙상블 모델을 통한 회귀 예측

##### 스태킹(Stacking)

- 개별적인 여러 알고리즘을 서로 결합해서 예측 결과를 도출한다는 점에서는
- 배깅 및 부스팅 방식과 동일하지만
- 큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로
- 다시 예측을 수행하는 것

##### 메타 모델

- 개별 모델의 예측된 데이터 세트를 기반
- 다시 학습 하고 예측 하는 방식

##### 스태킹 모델의 핵심

- 여러 개별 모델의 예측 데이터를
- 각각 스태킹 형태로 결합해
- 최종 메타 모델의 학습용 피처 데이터 세트와 테스트용 피처 데이터 세트를 만드는 것

###### 최종 메타 모델이 학습할 피처 데이터 세트

- 원본 학습 피처 데이터 세트로 학습한 개별 모델의 예측값을
- 스태킹 형태로 결합한 것

