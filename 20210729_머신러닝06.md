#### 회귀(Regression)

- 여러개의 독립변수와 한개의 종속변수간의 상관관계를 모델링하는 기법
- Y = W1X1 + W2X2 + W3X3 + ... + WnXn
- 독립변수(X) : 서로 간 상관관계가 적고 독립적인 변수
  - 피처 :아파트 크기, 방 개수, 주변 지하철역 수, 주변 학군
- 종속변수(Y)
  - 결정값 : 아파트 가격
- 회귀 계수(W) : 독립 변수의 값에 영향을 미치는 것



#### 선형회귀

- 가장 많이 사용되는 회귀
- 실제값과 예측값의 차이(오류의 제곱값)을 최소화하는 직선형 회귀선을 최적화하는 방식
- 실제값과 예측값의 차이인 오류를 최소로 줄일 수 있는 선형 함수를 찾아서
- 이 선형 함수에 독립변수(피처)를 입력해
- 종속변수(타깃값,예측값)을 예측하는 것
- 규제 방법에 따라 여러 유형으로 나뉨



#### 규제

- 일반적인 선형 회귀의 과적합 문제를 해결
- 회귀계수에 패널티 값을 적용한는 것



#### 대표적인 선형 회귀 모델

- 일반 선형 회귀 : 규제를 적용하지 않은 모델로 예측값과 실제값의 손실함수(RSS)를 최소화할 수 있도록 최적화
- 릿지(Ridge) : 선형 회귀에 L2규제를 추가한 회귀 모델
- 라쏘(Lasso) : 선형 회귀에 L1규제를 적용한 모델
- 엘라스틱넷(ElasticNet) : L2, L1 규제를 함께 결합한 모델
- 로지스틱 회귀(Rogistic Regression) : 분류에서 사용되는 선형 모델



#### 최적의 회귀 모델

- 실제값과 모델 사이의 오류값(잔차) 합이 최소가 되는 모델
- 오류값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는 것



#### 회귀평가 지표

##### MSE (Mean Squared Error)

- 실제값과 예측값의 차이를 제곱해서 평균한 것

##### RMSE (Root Meat Squared Error)

- MSE 값이 오류의 제곱을 구함
- 실제 오류 평균보다 더 커지는 특성이 있어
- MSE 에 루트를 씌운 것
- MSE 에 제곱근 씌워서 계산하는 함수 직접 만들어서 사용
- 값이 작을수록 성능이 좋음

##### R제곱 : 분산 기반으로 예측 성능 평가

- 실제값의 분산 대비 예측값의 분산 비율을 지표로 하며
- 1에 가까울수록 예측 정확도가 높음



#### LinearRegression 클래스를 이용해서 보스턴 주택 가격의 회귀 모델 생성

- train_test_split()을 이용해서 학습/테스트 데이터셋 분리
- fit() 학습, predict() 예측 수행
- metrics 모듈의 mean_squared_error() 이용해서 MSE 측정
- r2_score() 이용해서 R2 Score 측정



### 다항 회귀(Polynomial Regression)와 오버피팅/언더피팅



#### 단항 회귀

- 독립변수(피처)와 종속변수(타킷)의 관계가 일차 방정식 형태

#### 다항 회귀

- 회귀가 '독립변수'의 단항식이 아닌 2차,3차 방정식과 같은 다항식으로 표현
- 선형 회귀 (비선형 회귀 아님)
- 선형/비선형 구분은 회귀계수가 선형/비선형 여부에 따른 것이지 독립변수의 선형 / 비선형 여부와는 관계없음
- 피처 X에 대해 타깃 Y값의 관계를 단순 선형회귀 직선형으로 표현
- 다항 회귀 곡선형으로 표현한 것이 더 예측 성능이 높음![그림38](../../../study/ML/그림예제/그림38.PNG)

#### PolynomialFeatures 클래스 

- PolynomialFeatures 클래스를 통해서 피처를 다항식 피처로 변환
- PolynomialFeatures(degree=2) : 2차 다항식
- fit() / transform() 메서드를 사용해서 변환 작업 수행

```python
# PolynomialFeatures를 이용하여
# degree = 2인 2차 다항식으로 변환
poly = PolynomialFeatures(degree=2)
poly.fit(X)
poly_ftr = poly.transform(X)
print('변환된 2차 다항식 계수 features : \n', poly_ftr)

# transform (x1, x2) to (1, x1, x2, x1^2, x1*x2, x2^2)
```

- 다항식 값이 높아질수록 오버피팅을 급격하게 진행하고 오차가 늘어나게 됨

- 다항식 변환해서 사용할 때는 degree에 대해 주의를 해야함



### 편향-분산 트레이드 오프 (Bias_Variance Trade Off)

####  편향-분산 트레이드 오프

- 머신러닝이 극복해야 할 가장 중요한 이슈 중의 하나

- 일반적으로 편향과 분산은 한 쪽이 높으면 한 쪽이 낮아지는 경향이 있음

- 즉, 편향이 높으면 분산이 낮아지고 (과소적합)

- 반대로 분산이 높으면 편향아 낮아짐 (과적합)

  

#### 편향-분산 트레이드 오프 고려한 예측 모델
- 고편향/저분산 : 과속적합
- 저편향/고분산 : 과적합
- 편향과 분산이 서로 트레이드 오프를 이루면서
- 오류 Cost 값이 최대로 낮아지는 모델을 구축하는 것이
- 가장 효율적인 머신러닝 예측 모델을 만드는 방법



### 규제 선형 모델 - 릿지, 라쏘, 엘라스틱넷

Regularized Liner Models - Ridge, Lasso, Elastic Net



최적 모델을 위한 비용 함수 구성요소 = 학습 데이터 자차 오류 최소화 + 회귀 계수 크기 제어

-> 학습 데이터 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터 사용 (alpha)

alpha : 학습 데이터 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터



#### 규제

- 비용함수에 alpha 값으로 패널티를 부여해서

- 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식

- L2방식과 L1방식

  

#### L2 규제

- W의 제곱에 대해 패널티를 부여하는 방식

- 회귀 계수의 크기 감소

- L2 규제를 적용한 회귀 : 릿지(Redge) 방식

  

#### L1 규제

- W의 절대값에 대해 패널티를 부여하는 방식

- 영향력이 크지 않은 회귀 계수를 0으로 변환하고 제거

- 적절한 피처만 회귀에 포함시키는 피선 선택의 특성

- L1 규제를 적용한 회귀 : 라쏘(Lasso) 회귀

  

#### Elastic Net : L2, L1 규제를 함께 결합한 모델

- 주로 피처가 많은 데이터 세트에서 적용
- L1 규제로 피처의 개수를 줄임과 동시에
- L2 규제로 계수 값의 크기 조정



#### 규제 선형 회귀의 가장 대표적인 기법인 릿지, 라쏘, 엘라스틱넷 회귀 결론

- 어떤 것이 가장 좋은지는 상황에 따라 다름
- 각 알로리즘에서 하이퍼 파라미터를 변경해 가면서
- 최적의 예측 성능을 찾아내야 함
- 그러나 선형 회귀의 경우 최적의 하이퍼 파라미터를 찾아내는 것 못지 않게
- 먼저 데이터 분포도의 정규화와 인코딩 방법이 매우 중요함
